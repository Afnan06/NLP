# -*- coding: utf-8 -*-
"""NLP Sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180bTz0Hyp_mudugehjfu5dhsv1KYtLP6
"""

import ndjson
import pandas as pd
import numpy as np
import seaborn as sns

# reading reviews from json file
with open('/content/drive/My Drive/MACHINE LEARNING/NLP/Video_Games_5.json') as f:
    data = ndjson.load(f)

reviews=pd.DataFrame(data)
reviews.head()

reviews.shape

reviews.info

sns.countplot(data=reviews,x="overall")

#no of products
len(reviews['asin'].value_counts(dropna=False))

"""**SAMPLING REVIEWS**
Taking a random sample of the reviews by selecting 1500 reviews with rating 1, 500-500-500 reviews with ratings 2, 3, 4, and 1500 reviews with rating 5. This way you get a smaller balanced corpus, on which you will during Milestones 2-4.
"""

one_1500 = reviews[reviews['overall']==1.0].sample(n=1500)
two_500 = reviews[reviews['overall']==2.0].sample(n=500)
three_500 = reviews[reviews['overall']==3.0].sample(n=500)
four_500 = reviews[reviews['overall']==4.0].sample(n=500)
five_1500 = reviews[reviews['overall']==5.0].sample(n=1500)

undersampled_reviews = pd.concat([one_1500, two_500, three_500, four_500, five_1500], axis=0)

undersampled_reviews['overall'].value_counts(dropna=False)

sns.countplot(data=undersampled_reviews, x='overall')

"""**100k random samples**"""

sample_100K_revs = reviews.sample(n=100000, random_state=42)

sample_100K_revs.to_csv('/content/drive/My Drive/MACHINE LEARNING/NLP/Sentiment Analysis/sample_100K_revs', index=False)
undersampled_reviews.to_csv('/content/drive/My Drive/MACHINE LEARNING/NLP/Sentiment Analysis/undersampled_reviews', index=False)

"""**now model and preprocessing**"""

#sent_tokenize() method to split a document or paragraph 
#The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. 
# predict the correct tag for a given set of features. It is a process of converting a sentence to forms â€“ list of words, list of tuples 
#(where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag
#Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet.
# Synset instances are the groupings of synonymous words that express the same concept.

def penn_to_wn(tag):
    """
        Convert between the PennTreebank tags to simple Wordnet tags
    """
    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB
    return None

from nltk import sent_tokenize, pos_tag
from nltk.tokenize import TreebankWordTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from nltk.corpus import sentiwordnet as swn
from nltk.sentiment.util import mark_negation
from string import punctuation
from IPython.display import display
import pandas as pd
import nltk
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

def get_sentiment_score(text):
    
    """
        This method returns the sentiment score of a given text using SentiWordNet sentiment scores.
        input: text
        output: numeric (double) score, >0 means positive sentiment and <0 means negative sentiment.
    """    
    total_score = 0
    #print(text)
    raw_sentences = sent_tokenize(text)
    #print(raw_sentences)
    
    for sentence in raw_sentences:

        sent_score = 0     
        sentence = str(sentence)
        #print(sentence)
        sentence = sentence.replace("<br />"," ").translate(str.maketrans('','',punctuation)).lower()
        tokens = TreebankWordTokenizer().tokenize(text)
        tags = pos_tag(tokens)
        for word, tag in tags:
            wn_tag = penn_to_wn(tag)
            if not wn_tag:
                continue
            lemma = WordNetLemmatizer().lemmatize(word, pos=wn_tag)
            if not lemma:
                continue
            synsets = wn.synsets(lemma, pos=wn_tag)
            if not synsets:
                continue
            synset = synsets[0]
            swn_synset = swn.senti_synset(synset.name())
            sent_score += swn_synset.pos_score() - swn_synset.neg_score()

        total_score = total_score + (sent_score / len(tokens))

    
    return (total_score / len(raw_sentences)) * 100

reviews_ = pd.read_csv('/content/drive/My Drive/MACHINE LEARNING/NLP/Sentiment Analysis/undersampled_reviews')

reviews_.head()

nltk.download('sentiwordnet')

#now work on just review column so take it
reviews_.dropna(subset=['reviewText'], inplace=True)
#add score column 

reviews_['swn_score'] = reviews_['reviewText'].apply(lambda text : get_sentiment_score(text))
reviews_.head()

reviews_[['reviewText','swn_score']].sample(2)

reviews_['swn_sentiment'] = reviews_['swn_score'].apply(lambda x: "positive" if x>1 else ("negative" if x<0.5 else "neutral"))

reviews_['swn_sentiment'].value_counts(dropna=False)

sns.countplot(x='overall', hue='swn_sentiment' ,data = reviews_)

reviews_['true_sentiment'] = \
    reviews_['overall'].apply(lambda x: "positive" if x>=4 else ("neutral" if x==3 else "negative"))

y_swn_pred=reviews_['swn_sentiment'].tolist()
y_true = reviews_['true_sentiment'].tolist()

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_swn_pred)

fig , ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))
sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True, ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('True');

from sklearn.metrics import confusion_matrix,classification_report
target_names=[1,2,3]
print(classification_report(y_true, y_swn_pred))